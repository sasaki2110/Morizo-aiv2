#!/usr/bin/env python3
"""
05_3_test_llm.py

LLMServiceã®å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ
- ã‚¿ã‚¹ã‚¯åˆ†è§£æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
- å›ç­”æ•´å½¢æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
- åˆ¶ç´„è§£æ±ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
- ãƒ„ãƒ¼ãƒ«èª¬æ˜å–å¾—ã®ãƒ†ã‚¹ãƒˆ
- å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import os
import sys
from typing import Dict, Any, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
from dotenv import load_dotenv
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°ã®åˆæœŸåŒ–
from config.logging import setup_logging
setup_logging()

from config.loggers import GenericLogger

# ã‚µãƒ¼ãƒ“ã‚¹å±¤ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.llm_service import LLMService
from services.tool_router import ToolRouter

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = GenericLogger("test", "llm_service")


async def test_decompose_tasks():
    """ã‚¿ã‚¹ã‚¯åˆ†è§£æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ§ª [LLM_TEST] Starting decompose_tasks test...")
    print("ğŸ§ª [LLM_TEST] Starting decompose_tasks test...")
    
    try:
        # LLMServiceåˆæœŸåŒ–
        llm_service = LLMService()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        user_request = "åœ¨åº«ã‚’ç¢ºèªã—ã¦ã€çŒ®ç«‹ã‚’ææ¡ˆã—ã¦ãã ã•ã„"
        available_tools = ["inventory_list", "generate_menu_plan_with_history"]
        user_id = "test_user_123"
        
        # ã‚¿ã‚¹ã‚¯åˆ†è§£å®Ÿè¡Œ
        result = await llm_service.decompose_tasks(
            user_request=user_request,
            available_tools=available_tools,
            user_id=user_id
        )
        
        # çµæœç¢ºèª
        if isinstance(result, list) and len(result) > 0:
            logger.info(f"âœ… [LLM_TEST] decompose_tasks completed successfully: {len(result)} tasks")
            print(f"âœ… [LLM_TEST] decompose_tasks completed successfully: {len(result)} tasks")
            
            # ã‚¿ã‚¹ã‚¯ã®è©³ç´°ã‚’è¡¨ç¤º
            for i, task in enumerate(result, 1):
                print(f"  Task {i}: {task.get('description', 'N/A')}")
                logger.info(f"ğŸ“‹ [LLM_TEST] Task {i}: {task.get('description', 'N/A')}")
            
            return True
        else:
            logger.error(f"âŒ [LLM_TEST] decompose_tasks failed: empty result")
            print(f"âŒ [LLM_TEST] decompose_tasks failed: empty result")
            return False
            
    except Exception as e:
        logger.error(f"âŒ [LLM_TEST] decompose_tasks test failed: {e}")
        print(f"âŒ [LLM_TEST] decompose_tasks test failed: {e}")
        return False


async def test_format_response():
    """å›ç­”æ•´å½¢æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ§ª [LLM_TEST] Starting format_response test...")
    print("ğŸ§ª [LLM_TEST] Starting format_response test...")
    
    try:
        # LLMServiceåˆæœŸåŒ–
        llm_service = LLMService()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        results = [
            {
                "task_id": "task_1",
                "success": True,
                "data": {"inventory_count": 5}
            },
            {
                "task_id": "task_2", 
                "success": True,
                "data": {"menu_plan": "å’Œé£Ÿã®çŒ®ç«‹"}
            }
        ]
        
        # å›ç­”æ•´å½¢å®Ÿè¡Œ
        formatted_response = await llm_service.format_response(results)
        
        # çµæœç¢ºèª
        if isinstance(formatted_response, str) and len(formatted_response) > 0:
            logger.info(f"âœ… [LLM_TEST] format_response completed successfully")
            print(f"âœ… [LLM_TEST] format_response completed successfully")
            print(f"  Formatted response: {formatted_response}")
            logger.info(f"ğŸ“ [LLM_TEST] Formatted response: {formatted_response}")
            return True
        else:
            logger.error(f"âŒ [LLM_TEST] format_response failed: empty result")
            print(f"âŒ [LLM_TEST] format_response failed: empty result")
            return False
            
    except Exception as e:
        logger.error(f"âŒ [LLM_TEST] format_response test failed: {e}")
        print(f"âŒ [LLM_TEST] format_response test failed: {e}")
        return False


async def test_solve_constraints():
    """åˆ¶ç´„è§£æ±ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ§ª [LLM_TEST] Starting solve_constraints test...")
    print("ğŸ§ª [LLM_TEST] Starting solve_constraints test...")
    
    try:
        # LLMServiceåˆæœŸåŒ–
        llm_service = LLMService()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        candidates = [
            {"name": "option_1", "score": 0.8},
            {"name": "option_2", "score": 0.6},
            {"name": "option_3", "score": 0.9}
        ]
        constraints = {"min_score": 0.7}
        
        # åˆ¶ç´„è§£æ±ºå®Ÿè¡Œ
        result = await llm_service.solve_constraints(candidates, constraints)
        
        # çµæœç¢ºèª
        if isinstance(result, dict) and "selected" in result:
            logger.info(f"âœ… [LLM_TEST] solve_constraints completed successfully")
            print(f"âœ… [LLM_TEST] solve_constraints completed successfully")
            print(f"  Selected: {result.get('selected', {})}")
            print(f"  Reason: {result.get('reason', 'N/A')}")
            logger.info(f"ğŸ¯ [LLM_TEST] Selected: {result.get('selected', {})}")
            logger.info(f"ğŸ’­ [LLM_TEST] Reason: {result.get('reason', 'N/A')}")
            return True
        else:
            logger.error(f"âŒ [LLM_TEST] solve_constraints failed: invalid result")
            print(f"âŒ [LLM_TEST] solve_constraints failed: invalid result")
            return False
            
    except Exception as e:
        logger.error(f"âŒ [LLM_TEST] solve_constraints test failed: {e}")
        print(f"âŒ [LLM_TEST] solve_constraints test failed: {e}")
        return False


async def test_get_available_tools_description():
    """ãƒ„ãƒ¼ãƒ«èª¬æ˜å–å¾—ã®ãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ§ª [LLM_TEST] Starting get_available_tools_description test...")
    print("ğŸ§ª [LLM_TEST] Starting get_available_tools_description test...")
    
    try:
        # LLMServiceåˆæœŸåŒ–
        llm_service = LLMService()
        
        # ToolRouteråˆæœŸåŒ–
        tool_router = ToolRouter()
        
        # ãƒ„ãƒ¼ãƒ«èª¬æ˜å–å¾—å®Ÿè¡Œ
        description = llm_service.get_available_tools_description(tool_router)
        
        # çµæœç¢ºèª
        if isinstance(description, str) and len(description) > 0:
            logger.info(f"âœ… [LLM_TEST] get_available_tools_description completed successfully")
            print(f"âœ… [LLM_TEST] get_available_tools_description completed successfully")
            
            # èª¬æ˜ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            lines = description.split('\n')
            print(f"  Description preview ({len(lines)} lines):")
            for i, line in enumerate(lines[:5], 1):  # æœ€åˆã®5è¡Œã‚’è¡¨ç¤º
                print(f"    {i}. {line}")
            
            logger.info(f"ğŸ“‹ [LLM_TEST] Description length: {len(description)} characters")
            return True
        else:
            logger.error(f"âŒ [LLM_TEST] get_available_tools_description failed: empty result")
            print(f"âŒ [LLM_TEST] get_available_tools_description failed: empty result")
            return False
            
    except Exception as e:
        logger.error(f"âŒ [LLM_TEST] get_available_tools_description test failed: {e}")
        print(f"âŒ [LLM_TEST] get_available_tools_description test failed: {e}")
        return False


async def test_create_dynamic_prompt():
    """å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ§ª [LLM_TEST] Starting create_dynamic_prompt test...")
    print("ğŸ§ª [LLM_TEST] Starting create_dynamic_prompt test...")
    
    try:
        # LLMServiceåˆæœŸåŒ–
        llm_service = LLMService()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        base_prompt = "ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚"
        tool_descriptions = "åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n- inventory_list: åœ¨åº«ä¸€è¦§ã‚’å–å¾—\n- generate_menu_plan: çŒ®ç«‹ã‚’ç”Ÿæˆ"
        user_context = {
            "user_id": "test_user_123",
            "session_id": "session_456",
            "timestamp": "2025-01-29 10:00:00"
        }
        
        # å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Ÿè¡Œ
        dynamic_prompt = llm_service.create_dynamic_prompt(
            base_prompt=base_prompt,
            tool_descriptions=tool_descriptions,
            user_context=user_context
        )
        
        # çµæœç¢ºèª
        if isinstance(dynamic_prompt, str) and len(dynamic_prompt) > len(base_prompt):
            logger.info(f"âœ… [LLM_TEST] create_dynamic_prompt completed successfully")
            print(f"âœ… [LLM_TEST] create_dynamic_prompt completed successfully")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            print(f"  Dynamic prompt preview:")
            print(f"    {dynamic_prompt[:200]}...")
            
            logger.info(f"ğŸ“ [LLM_TEST] Dynamic prompt length: {len(dynamic_prompt)} characters")
            return True
        else:
            logger.error(f"âŒ [LLM_TEST] create_dynamic_prompt failed: invalid result")
            print(f"âŒ [LLM_TEST] create_dynamic_prompt failed: invalid result")
            return False
            
    except Exception as e:
        logger.error(f"âŒ [LLM_TEST] create_dynamic_prompt test failed: {e}")
        print(f"âŒ [LLM_TEST] create_dynamic_prompt test failed: {e}")
        return False


async def test_error_handling():
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ§ª [LLM_TEST] Starting error handling test...")
    print("ğŸ§ª [LLM_TEST] Starting error handling test...")
    
    try:
        # LLMServiceåˆæœŸåŒ–
        llm_service = LLMService()
        
        # ç©ºã®ãƒªã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ
        empty_results = await llm_service.format_response([])
        if isinstance(empty_results, str):
            logger.info(f"âœ… [LLM_TEST] Empty results handling: OK")
            print(f"âœ… [LLM_TEST] Empty results handling: OK")
        else:
            logger.error(f"âŒ [LLM_TEST] Empty results handling: FAILED")
            print(f"âŒ [LLM_TEST] Empty results handling: FAILED")
            return False
        
        # ç©ºã®å€™è£œãƒªã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ
        empty_candidates_result = await llm_service.solve_constraints([], {})
        if isinstance(empty_candidates_result, dict) and "selected" in empty_candidates_result:
            logger.info(f"âœ… [LLM_TEST] Empty candidates handling: OK")
            print(f"âœ… [LLM_TEST] Empty candidates handling: OK")
        else:
            logger.error(f"âŒ [LLM_TEST] Empty candidates handling: FAILED")
            print(f"âŒ [LLM_TEST] Empty candidates handling: FAILED")
            return False
        
        logger.info(f"âœ… [LLM_TEST] Error handling test completed successfully")
        print(f"âœ… [LLM_TEST] Error handling test completed successfully")
        return True
        
    except Exception as e:
        logger.error(f"âŒ [LLM_TEST] Error handling test failed: {e}")
        print(f"âŒ [LLM_TEST] Error handling test failed: {e}")
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    logger.info("ğŸš€ [MAIN] Starting LLMService tests...")
    print("ğŸš€ [MAIN] Starting LLMService tests...")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_results = []
    
    # 1. ã‚¿ã‚¹ã‚¯åˆ†è§£ãƒ†ã‚¹ãƒˆ
    test1_result = await test_decompose_tasks()
    test_results.append(("Decompose Tasks", test1_result))
    
    # 2. å›ç­”æ•´å½¢ãƒ†ã‚¹ãƒˆ
    test2_result = await test_format_response()
    test_results.append(("Format Response", test2_result))
    
    # 3. åˆ¶ç´„è§£æ±ºãƒ†ã‚¹ãƒˆ
    test3_result = await test_solve_constraints()
    test_results.append(("Solve Constraints", test3_result))
    
    # 4. ãƒ„ãƒ¼ãƒ«èª¬æ˜å–å¾—ãƒ†ã‚¹ãƒˆ
    test4_result = await test_get_available_tools_description()
    test_results.append(("Get Tools Description", test4_result))
    
    # 5. å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    test5_result = await test_create_dynamic_prompt()
    test_results.append(("Create Dynamic Prompt", test5_result))
    
    # 6. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    test6_result = await test_error_handling()
    test_results.append(("Error Handling", test6_result))
    
    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("ğŸ“Š [MAIN] LLMService Test Results Summary:")
    print("ğŸ“Š [MAIN] LLMService Test Results Summary:")
    
    passed_tests = 0
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        logger.info(f"  - {test_name}: {status}")
        print(f"  - {test_name}: {status}")
        if result:
            passed_tests += 1
    
    success_rate = (passed_tests / total_tests) * 100
    logger.info(f"ğŸ“ˆ Test Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
    print(f"ğŸ“ˆ Test Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
    
    if passed_tests == total_tests:
        logger.info("ğŸ‰ [MAIN] All LLMService tests passed!")
        print("ğŸ‰ [MAIN] All LLMService tests passed!")
    elif passed_tests >= total_tests * 0.8:
        logger.info("âœ… [MAIN] Most LLMService tests passed!")
        print("âœ… [MAIN] Most LLMService tests passed!")
    else:
        logger.error("âŒ [MAIN] Some LLMService tests failed. Please check the logs.")
        print("âŒ [MAIN] Some LLMService tests failed. Please check the logs.")
    
    return passed_tests == total_tests


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(main())
