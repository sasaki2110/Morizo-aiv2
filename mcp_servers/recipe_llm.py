"""
Morizo AI v2 - Recipe LLM Client

This module provides LLM-based recipe title generation functionality.
"""

import os
import asyncio
from typing import Dict, Any, List, Optional
from openai import AsyncOpenAI
from dotenv import load_dotenv

from config.loggers import GenericLogger, log_prompt_with_tokens

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


class RecipeLLM:
    """LLMæ¨è«–ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.logger = GenericLogger("mcp", "recipe_llm", initialize_logging=False)
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
        self.temperature = float(os.getenv('OPENAI_TEMPERATURE', '0.8'))
        
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY is required")
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.client = AsyncOpenAI(api_key=self.api_key)
        
        self.logger.info(f"ğŸ¤– [LLM] Initialized with model: {self.model}, temperature: {self.temperature}")
    
    # é£Ÿæé‡è¤‡æŠ‘æ­¢æ©Ÿèƒ½
    # - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§ã€Œé£Ÿæã®é‡è¤‡ã‚’é¿ã‘ã‚‹ã€ã¨æ˜ç¤ºçš„ã«æŒ‡ç¤º
    # - LLMãŒ1å›ã®æ¨è«–ã§ä¸»èœãƒ»å‰¯èœãƒ»æ±ç‰©ã®3å“æ§‹æˆã‚’ç”Ÿæˆ
    # - å„æ–™ç†é–“ã§é£Ÿæã®é‡è¤‡ã‚’é¿ã‘ã‚‹ã‚ˆã†ã«è¨­è¨ˆ
    # - åœ¨åº«é£Ÿæã‚’æœ€å¤§é™æ´»ç”¨ã—ã€ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„çŒ®ç«‹æ§‹æˆã‚’å®Ÿç¾
    
    async def generate_menu_titles(
        self, 
        inventory_items: List[str], 
        menu_type: str,
        excluded_recipes: List[str] = None
    ) -> Dict[str, Any]:
        """
        LLMæ¨è«–ã«ã‚ˆã‚‹ç‹¬å‰µçš„ãªçŒ®ç«‹ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
        
        Args:
            inventory_items: åœ¨åº«é£Ÿæãƒªã‚¹ãƒˆ
            menu_type: çŒ®ç«‹ã®ã‚¿ã‚¤ãƒ—
            excluded_recipes: é™¤å¤–ã™ã‚‹ãƒ¬ã‚·ãƒ”ã‚¿ã‚¤ãƒˆãƒ«
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸçŒ®ç«‹ã‚¿ã‚¤ãƒˆãƒ«ã®å€™è£œãƒªã‚¹ãƒˆ
        
        å®Ÿè£…æ¸ˆã¿: é£Ÿæé‡è¤‡æŠ‘æ­¢æ©Ÿèƒ½
        - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§ã€Œé£Ÿæã®é‡è¤‡ã‚’é¿ã‘ã‚‹ã€ã¨æ˜ç¤ºçš„ã«æŒ‡ç¤ºï¼ˆ_build_menu_promptå‚ç…§ï¼‰
        - 1å›ã®LLMæ¨è«–ã§ä¸»èœãƒ»å‰¯èœãƒ»æ±ç‰©ã®3å“æ§‹æˆã‚’ç”Ÿæˆ
        - å„æ–™ç†é–“ã§é£ŸæãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        - LLMã®æ¨è«–èƒ½åŠ›ã«ã‚ˆã‚Šã€çŒ®ç«‹å†…ã®é£Ÿæãƒãƒ©ãƒ³ã‚¹ã‚’è‡ªå‹•èª¿æ•´
        """
        try:
            self.logger.info(f"ğŸ§  [LLM] Generating menu titles for {menu_type} with {len(inventory_items)} ingredients")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            prompt = self._build_menu_prompt(inventory_items, menu_type, excluded_recipes)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ã‚®ãƒ³ã‚°
            log_prompt_with_tokens(prompt, max_tokens=1000, logger_name="mcp.recipe_llm")
            
            # LLMå‘¼ã³å‡ºã—
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=1000
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            menu_titles = self._parse_menu_response(response.choices[0].message.content)
            
            self.logger.info(f"âœ… [LLM] Generated {len(menu_titles)} menu titles")
            return {"success": True, "data": menu_titles}
            
        except Exception as e:
            self.logger.error(f"âŒ [LLM] Failed to generate menu titles: {e}")
            return {"success": False, "error": str(e)}
    
    def _build_menu_prompt(
        self, 
        inventory_items: List[str], 
        menu_type: str,
        excluded_recipes: List[str] = None
    ) -> str:
        """çŒ®ç«‹ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        
        excluded_text = ""
        if excluded_recipes:
            excluded_text = f"\né™¤å¤–ã™ã‚‹ãƒ¬ã‚·ãƒ”: {', '.join(excluded_recipes)}"
        
        prompt = f"""
åœ¨åº«é£Ÿæ: {', '.join(inventory_items)}
çŒ®ç«‹ã‚¿ã‚¤ãƒ—: {menu_type}{excluded_text}

ä»¥ä¸‹ã®æ¡ä»¶ã§ç‹¬å‰µçš„ãªçŒ®ç«‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:
1. ä¸»èœãƒ»å‰¯èœãƒ»æ±ç‰©ã®3å“æ§‹æˆ
2. åœ¨åº«é£Ÿæã®ã¿ã‚’ä½¿ç”¨
3. é£Ÿæã®é‡è¤‡ã‚’é¿ã‘ã‚‹
4. ç‹¬å‰µçš„ã§æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã‚¿ã‚¤ãƒˆãƒ«
5. é™¤å¤–ãƒ¬ã‚·ãƒ”ã¯ä½¿ç”¨ã—ãªã„

é‡è¦: å…·ä½“çš„ãªèª¿ç†æ‰‹é †ã¯ç”Ÿæˆã›ãšã€ãƒ¬ã‚·ãƒ”ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ä¾‹: "ç‰›ä¹³ã¨åµã®ãƒ•ãƒ¬ãƒ³ãƒãƒˆãƒ¼ã‚¹ãƒˆ"ã€"ã»ã†ã‚Œã‚“è‰ã®èƒ¡éº»å’Œãˆ"

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{
    "main_dish": "ä¸»èœã®ã‚¿ã‚¤ãƒˆãƒ«",
    "side_dish": "å‰¯èœã®ã‚¿ã‚¤ãƒˆãƒ«", 
    "soup": "æ±ç‰©ã®ã‚¿ã‚¤ãƒˆãƒ«",
    "ingredients_used": ["ä½¿ç”¨é£Ÿæ1", "ä½¿ç”¨é£Ÿæ2", ...]
}}

ç”Ÿæˆã™ã‚‹çŒ®ç«‹:
"""
        return prompt
    
    def _parse_menu_response(self, response_content: str) -> Dict[str, Any]:
        """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æã—ã¦çŒ®ç«‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        try:
            import json
            
            # JSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            menu_data = json.loads(response_content.strip())
            
            return {
                "main_dish": menu_data.get("main_dish", ""),
                "side_dish": menu_data.get("side_dish", ""),
                "soup": menu_data.get("soup", ""),
                "ingredients_used": menu_data.get("ingredients_used", [])
            }
            
        except json.JSONDecodeError:
            # JSONè§£æã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã‚’è©¦è¡Œ
            self.logger.warning("âš ï¸ [LLM] Failed to parse JSON response, attempting text extraction")
            return self._extract_from_text(response_content)
        except Exception as e:
            self.logger.error(f"âŒ [LLM] Failed to parse response: {e}")
            return {"main_dish": "", "side_dish": "", "soup": "", "ingredients_used": []}
    
    def _extract_from_text(self, text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çŒ®ç«‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        # ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆè§£æï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šé«˜åº¦ãªè§£æãŒå¿…è¦ï¼‰
        lines = text.strip().split('\n')
        
        main_dish = ""
        side_dish = ""
        soup = ""
        ingredients = []
        
        for line in lines:
            line = line.strip()
            if "ä¸»èœ" in line:
                main_dish = line.replace("ä¸»èœ:", "").replace("ä¸»èœï¼š", "").strip()
            elif "å‰¯èœ" in line:
                side_dish = line.replace("å‰¯èœ:", "").replace("å‰¯èœï¼š", "").strip()
            elif "æ±ç‰©" in line:
                soup = line.replace("æ±ç‰©:", "").replace("æ±ç‰©ï¼š", "").strip()
        
        return {
            "main_dish": main_dish,
            "side_dish": side_dish,
            "soup": soup,
            "ingredients_used": ingredients
        }
    
    async def generate_main_dish_candidates(
        self, 
        inventory_items: List[str], 
        menu_type: str,
        main_ingredient: str = None,  # ä¸»è¦é£Ÿæ
        excluded_recipes: List[str] = None,
        count: int = 2
    ) -> Dict[str, Any]:
        """ä¸»èœå€™è£œã‚’è¤‡æ•°ä»¶ç”Ÿæˆï¼ˆä¸»è¦é£Ÿæè€ƒæ…®ï¼‰"""
        
        main_ingredient_text = ""
        if main_ingredient:
            main_ingredient_text = f"\né‡è¦: {main_ingredient}ã‚’å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        
        # é™¤å¤–ãƒ¬ã‚·ãƒ”ã®è¿½åŠ 
        excluded_text = ""
        if excluded_recipes:
            excluded_text = f"\né™¤å¤–ãƒ¬ã‚·ãƒ”ï¼ˆææ¡ˆã—ãªã„ã§ãã ã•ã„ï¼‰: {', '.join(excluded_recipes)}"
        
        prompt = f"""
åœ¨åº«é£Ÿæ: {', '.join(inventory_items)}
çŒ®ç«‹ã‚¿ã‚¤ãƒ—: {menu_type}{main_ingredient_text}{excluded_text}

ä»¥ä¸‹ã®æ¡ä»¶ã§ä¸»èœã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’{count}ä»¶ç”Ÿæˆã—ã¦ãã ã•ã„:
1. åœ¨åº«é£Ÿæã®ã¿ã‚’ä½¿ç”¨
2. ç‹¬å‰µçš„ã§æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã‚¿ã‚¤ãƒˆãƒ«
3. é™¤å¤–ãƒ¬ã‚·ãƒ”ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„
4. å„ææ¡ˆã«ä½¿ç”¨é£Ÿæãƒªã‚¹ãƒˆã‚’å«ã‚ã‚‹

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{
    "candidates": [
        {{"title": "ä¸»èœã‚¿ã‚¤ãƒˆãƒ«1", "ingredients": ["é£Ÿæ1", "é£Ÿæ2"]}},
        {{"title": "ä¸»èœã‚¿ã‚¤ãƒˆãƒ«2", "ingredients": ["é£Ÿæ1", "é£Ÿæ3"]}}
    ]
}}
"""
        
        try:
            self.logger.info(f"ğŸ¤– [LLM] Generating {count} main dish candidates with main ingredient: {main_ingredient}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ã‚®ãƒ³ã‚°
            log_prompt_with_tokens(prompt, max_tokens=1000, logger_name="mcp.recipe_llm")
            
            # LLMå‘¼ã³å‡ºã—
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=1000
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            candidates = self._parse_main_dish_response(response.choices[0].message.content)
            
            self.logger.info(f"âœ… [LLM] Generated {len(candidates)} main dish candidates")
            return {"success": True, "data": {"candidates": candidates}}
            
        except Exception as e:
            self.logger.error(f"âŒ [LLM] Failed to generate main dish candidates: {e}")
            return {"success": False, "error": str(e)}

    def _parse_main_dish_response(self, response_content: str) -> List[Dict[str, Any]]:
        """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æã—ã¦ä¸»èœå€™è£œã‚’æŠ½å‡º"""
        try:
            import json
            import re
            
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)
                return data.get("candidates", [])
            
            return []
        except Exception as e:
            self.logger.error(f"âŒ [LLM] Failed to parse main dish response: {e}")
            return []


if __name__ == "__main__":
    print("âœ… Recipe LLM module loaded successfully")
