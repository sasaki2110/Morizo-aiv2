#!/usr/bin/env python3
"""
LLMService - LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹

LLMå‘¼ã³å‡ºã—ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹
åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦LLMæ©Ÿèƒ½ã‚’æä¾›
"""

from typing import Dict, Any, List, Optional
from config.loggers import GenericLogger
from .llm.prompt_manager import PromptManager
from .llm.response_processor import ResponseProcessor
from .llm.llm_client import LLMClient
from .llm.request_analyzer import RequestAnalyzer


class LLMService:
    """LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = GenericLogger("service", "llm")
        
        # åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
        self.prompt_manager = PromptManager()
        self.response_processor = ResponseProcessor()
        self.llm_client = LLMClient()
        
        # Phase 2.5A: RequestAnalyzer ã‚’è¿½åŠ 
        self.request_analyzer = RequestAnalyzer()
    
    async def decompose_tasks(
        self, 
        user_request: str, 
        available_tools: List[str], 
        user_id: str,
        sse_session_id: str = None,
        session_context: dict = None
    ) -> List[Dict[str, Any]]:
        """
        å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†è§£
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            sse_session_id: SSEã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            session_context: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            åˆ†è§£ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã€ã¾ãŸã¯æ›–æ˜§æ€§ç¢ºèªç”¨ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Decomposing tasks for user: {user_id}")
            self.logger.info(f"ğŸ“ [LLMService] User request: '{user_request}'")
            
            # Phase 2.5C: ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ†æï¼ˆRequestAnalyzer ã‚’ä½¿ç”¨ï¼‰
            analysis_result = self.request_analyzer.analyze(
                request=user_request,
                user_id=user_id,
                sse_session_id=sse_session_id,
                session_context=session_context or {}
            )
            
            self.logger.info(f"ğŸ” [LLMService] Analysis result: pattern={analysis_result['pattern']}")
            
            # æ›–æ˜§æ€§ãŒã‚ã‚‹å ´åˆã€ç¢ºèªè³ªå•ã‚’è¿”ã™
            if analysis_result["ambiguities"]:
                self.logger.info(f"âš ï¸ [LLMService] Ambiguity detected: {len(analysis_result['ambiguities'])} ambiguities")
                # TODO: æ›–æ˜§æ€§ç¢ºèªã®å®Ÿè£…ï¼ˆPhase 1Bå‚ç…§ï¼‰
                # ç¾æ™‚ç‚¹ã§ã¯æ—¢å­˜ã®å‡¦ç†ã‚’ç¶šè¡Œ
            
            # Phase 2.5C: å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆæ–°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å¼·åˆ¶ä½¿ç”¨ï¼‰
            from .llm.prompt_manager import PromptManager as NewPromptManager
            new_prompt_manager = NewPromptManager()
            
            try:
                prompt = new_prompt_manager.build_prompt(
                    analysis_result=analysis_result,
                    user_id=user_id,
                    sse_session_id=sse_session_id
                )
                self.logger.info(f"âœ… [LLMService] Dynamic prompt built using RequestAnalyzer (pattern={analysis_result['pattern']})")
            except Exception as e:
                import traceback
                self.logger.error(f"âŒ [LLMService] Failed to build dynamic prompt: {e}")
                self.logger.error(traceback.format_exc())
                # Phase 2.5Cå®Œäº†å¾Œã¯ã‚¨ãƒ©ãƒ¼ã‚’ä¾‹å¤–ã¨ã—ã¦æ‰±ã†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„ï¼‰
                raise
            
            # 2. OpenAI APIå‘¼ã³å‡ºã—
            response = await self.llm_client.call_openai_api(prompt)
            
            # 3. JSONè§£æ
            tasks = self.response_processor.parse_llm_response(response)
            
            # 4. ã‚¿ã‚¹ã‚¯å½¢å¼ã«å¤‰æ›
            converted_tasks = self.response_processor.convert_to_task_format(tasks, user_id)
            
            # ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"âœ… [LLMService] Tasks decomposed successfully: {len(converted_tasks)} tasks")
            for i, task in enumerate(converted_tasks, 1):
                self.logger.info(f"ğŸ“‹ [LLMService] Task {i}:")
                self.logger.info(f"  Service: {task.get('service')}")
                self.logger.info(f"  Method: {task.get('method')}")
                self.logger.info(f"  Parameters: {task.get('parameters')}")
                self.logger.info(f"  Dependencies: {task.get('dependencies')}")
            
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in decompose_tasks: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self.llm_client.get_fallback_tasks(user_id)
    
    async def format_response(
        self, 
        results: Dict[str, Any],
        sse_session_id: str = None
    ) -> tuple[str, Optional[Dict[str, Any]]]:
        """
        æœ€çµ‚å›ç­”æ•´å½¢
        
        Args:
            results: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœè¾æ›¸ (task1, task2, task3, task4)
            sse_session_id: SSEã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        
        Returns:
            (æ•´å½¢ã•ã‚ŒãŸå›ç­”, JSONå½¢å¼ã®ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿)
        """
        response, menu_data = await self.response_processor.format_final_response(results, sse_session_id)
        self.logger.info(f"ğŸ” [LLMService] Menu data received: {menu_data is not None}")
        if menu_data:
            self.logger.info(f"ğŸ“Š [LLMService] Menu data size: {len(str(menu_data))} characters")
        return response, menu_data
    
    def create_dynamic_prompt(
        self, 
        base_prompt: str, 
        tool_descriptions: str,
        user_context: Dict[str, Any]
    ) -> str:
        """
        å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        
        Args:
            base_prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            tool_descriptions: ãƒ„ãƒ¼ãƒ«èª¬æ˜
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # Phase 2.5C: æ–°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ï¼ˆcreate_dynamic_promptã¯å®Ÿè£…ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼‰
        return self.prompt_manager.create_dynamic_prompt_bak(base_prompt, tool_descriptions, user_context)
