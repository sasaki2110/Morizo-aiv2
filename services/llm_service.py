#!/usr/bin/env python3
"""
LLMService - LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹

LLMå‘¼ã³å‡ºã—ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆãƒ»ç®¡ç†ã¨å‹•çš„ãƒ„ãƒ¼ãƒ«å–å¾—ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‹•çš„åŸ‹ã‚è¾¼ã¿ã‚’æä¾›
"""

import os
import json
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from openai import AsyncOpenAI
from config.loggers import GenericLogger, log_prompt_with_tokens

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


class LLMService:
    """LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = GenericLogger("service", "llm")
        
        # OpenAIè¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.openai_temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.8"))
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if self.openai_api_key:
            self.openai_client = AsyncOpenAI(api_key=self.openai_api_key)
            self.logger.info(f"âœ… [LLMService] OpenAI client initialized with model: {self.openai_model}")
        else:
            self.openai_client = None
            self.logger.warning("âš ï¸ [LLMService] OPENAI_API_KEY not found, LLM calls will be disabled")
    
    def _build_planning_prompt(self, user_request: str) -> str:
        """
        ã‚¿ã‚¹ã‚¯åˆ†è§£ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰åå¯¾å¿œç‰ˆï¼‰
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        planning_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã‚’åˆ†æã—ã€é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚: "{user_request}"

åˆ©ç”¨å¯èƒ½ãªã‚µãƒ¼ãƒ“ã‚¹ã¨æ©Ÿèƒ½:

- **inventory_service**: åœ¨åº«ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹
  - `get_inventory()`: ç¾åœ¨ã®å…¨åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚
  - `add_inventory(item_name: str, quantity: float, ...)`: åœ¨åº«ã«æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ ã—ã¾ã™ã€‚
  - `update_inventory(item_identifier: str, updates: dict, strategy: str)`: åœ¨åº«æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚strategyã«ã¯ 'by_id', 'by_name', 'by_name_oldest', 'by_name_latest' ãŒæŒ‡å®šå¯èƒ½ã§ã™ã€‚
  - `delete_inventory(item_identifier: str, strategy: str)`: åœ¨åº«ã‚’å‰Šé™¤ã—ã¾ã™ã€‚strategyã«ã¯ 'by_id', 'by_name', 'by_name_oldest', 'by_name_latest' ãŒæŒ‡å®šå¯èƒ½ã§ã™ã€‚

- **recipe_service**: ãƒ¬ã‚·ãƒ”ãƒ»çŒ®ç«‹ã‚µãƒ¼ãƒ“ã‚¹
  - `generate_menu_plan(inventory_items: list, user_id: str, ...)`: åœ¨åº«ãƒªã‚¹ãƒˆã«åŸºã¥ãã€LLMã«ã‚ˆã‚‹ç‹¬å‰µçš„ãªçŒ®ç«‹ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚éå»ã®å±¥æ­´ã‚‚è€ƒæ…®ã—ã¾ã™ã€‚
  - `search_menu_from_rag(query: str, user_id: str, ...)`: RAGã‚’ä½¿ç”¨ã—ã¦éå»ã®çŒ®ç«‹å±¥æ­´ã‹ã‚‰é¡ä¼¼ã®çŒ®ç«‹ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
  - `search_recipes_from_web(recipe_name: str, ...)`: æŒ‡å®šã•ã‚ŒãŸæ–™ç†åã®ãƒ¬ã‚·ãƒ”ã‚’Webæ¤œç´¢ã—ã€URLã‚’å«ã‚€è©³ç´°æƒ…å ±ã‚’è¿”ã—ã¾ã™ã€‚
  - `get_recipe_history(user_id: str, ...)`: éå»ã®æ–™ç†å±¥æ­´ã‚’å–å¾—ã—ã¾ã™ã€‚

- **session_service**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆé€šå¸¸ã¯ç›´æ¥å‘¼ã³å‡ºã—ä¸è¦ï¼‰

**æœ€é‡è¦ãƒ«ãƒ¼ãƒ«: çŒ®ç«‹ç”Ÿæˆã®éš›ã®ã‚¿ã‚¹ã‚¯æ§‹æˆ**

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ãŒã€ŒçŒ®ç«‹ã€ã‚„ã€Œãƒ¬ã‚·ãƒ”ã€ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹å ´åˆã€å¿…ãšä»¥ä¸‹ã®4æ®µéšã®ã‚¿ã‚¹ã‚¯æ§‹æˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š

1. **task1**: `inventory_service.get_inventory()` ã‚’å‘¼ã³å‡ºã—ã€ç¾åœ¨ã®åœ¨åº«ã‚’ã™ã¹ã¦å–å¾—ã™ã‚‹ã€‚
2. **task2**: `recipe_service.generate_menu_plan()` ã‚’å‘¼ã³å‡ºã™ã€‚ãã®éš›ã€ã‚¹ãƒ†ãƒƒãƒ—1ã§å–å¾—ã—ãŸåœ¨åº«æƒ…å ±ã‚’ `inventory_items` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®šã™ã‚‹ã€‚
3. **task3**: `recipe_service.search_menu_from_rag()` ã‚’å‘¼ã³å‡ºã™ã€‚ãã®éš›ã€ã‚¹ãƒ†ãƒƒãƒ—1ã§å–å¾—ã—ãŸåœ¨åº«æƒ…å ±ã‚’ `inventory_items` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®šã™ã‚‹ã€‚
4. **task4**: `recipe_service.search_recipes_from_web()` ã‚’å‘¼ã³å‡ºã™ã€‚ãã®éš›ã€ã‚¹ãƒ†ãƒƒãƒ—2ã¨ã‚¹ãƒ†ãƒƒãƒ—3ã®çµæœã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹ã€‚

**ä¸¦åˆ—å®Ÿè¡Œã®æŒ‡ç¤º**: task2ã¨task3ã¯ä¸¦åˆ—ã§å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚dependenciesã«task1ã®ã¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

**çŒ®ç«‹ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãƒ«ãƒ¼ãƒ«**:
- task2ã¨task3ã®çµæœã¯è¾æ›¸å½¢å¼ã®çŒ®ç«‹ãƒ‡ãƒ¼ã‚¿ã§ã™ï¼ˆmain_dish, side_dish, soupãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€ï¼‰
- task4ã§ã¯ã€task2ã¨task3ã®ä¸¡æ–¹ã®çµæœã‚’çµ±åˆã—ã¦ãƒ¬ã‚·ãƒ”æ¤œç´¢ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
  - `"recipe_titles": ["task2.result.main_dish", "task2.result.side_dish", "task3.result.main_dish", "task3.result.side_dish"]`
  - ã¾ãŸã¯ã€ä¸»èœã®ã¿: `"recipe_titles": ["task2.result.main_dish", "task3.result.main_dish"]`

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ³¨å…¥ã®ãƒ«ãƒ¼ãƒ«**:
- å…ˆè¡Œã‚¿ã‚¹ã‚¯ã®çµæœã‚’å¾Œç¶šã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ³¨å…¥ã™ã‚‹å ´åˆã¯ã€å¿…ãš `"å…ˆè¡Œã‚¿ã‚¹ã‚¯å.result"` å½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- è¾æ›¸ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å‚ç…§ã®å ´åˆã¯ `"å…ˆè¡Œã‚¿ã‚¹ã‚¯å.result.ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å"` å½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- ä¾‹: task1ã®çµæœã‚’task2ã§ä½¿ç”¨ã™ã‚‹å ´åˆ â†’ `"inventory_items": "task1.result"`
- ä¾‹: task2ã®ä¸»èœã‚’task4ã§ä½¿ç”¨ã™ã‚‹å ´åˆ â†’ `"recipe_title": "task2.result.main_dish"`
- ã“ã®å½¢å¼ã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•çš„ã«å…ˆè¡Œã‚¿ã‚¹ã‚¯ã®çµæœã‚’å¾Œç¶šã‚¿ã‚¹ã‚¯ã«æ³¨å…¥ã—ã¾ã™ã€‚

**æ›–æ˜§ãªåœ¨åº«æ“ä½œã®æŒ‡ç¤ºã«ã¤ã„ã¦**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œå¤ã„æ–¹ã€ã€Œæœ€æ–°ã€ãªã©ã‚’æ˜ç¤ºã—ãªã„é™ã‚Šã€`update_inventory` ã‚„ `delete_inventory` ã® `strategy` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ `'by_name'` ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µãƒ¼ãƒ“ã‚¹å±¤ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç¢ºèªãƒ—ãƒ­ã‚»ã‚¹ãŒèµ·å‹•ã—ã¾ã™ã€‚
- ä¾‹: ã€Œç‰›ä¹³ã‚’å‰Šé™¤ã—ã¦ã€ â†’ `delete_inventory(item_identifier='ç‰›ä¹³', strategy='by_name')`
- ä¾‹: ã€Œå¤ã„ç‰›ä¹³ã‚’å‰Šé™¤ã—ã¦ã€ â†’ `delete_inventory(item_identifier='ç‰›ä¹³', strategy='by_name_oldest')`

**å‡ºåŠ›å½¢å¼**: å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã¯ç¦æ­¢ï¼‰ï¼š

{{
    "tasks": [
        {{
            "id": "task1",
            "description": "ã‚¿ã‚¹ã‚¯ã®è‡ªç„¶è¨€èªã§ã®èª¬æ˜",
            "service": "å‘¼ã³å‡ºã™ã‚µãƒ¼ãƒ“ã‚¹å",
            "method": "å‘¼ã³å‡ºã™ãƒ¡ã‚½ãƒƒãƒ‰å",
            "parameters": {{ "key": "value" }},
            "dependencies": []
        }}
    ]
}}

**çŒ®ç«‹ç”Ÿæˆã®å…·ä½“ä¾‹ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰åå¯¾å¿œï¼‰**:
{{
    "tasks": [
        {{
            "id": "task1",
            "description": "ç¾åœ¨ã®å…¨åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹",
            "service": "inventory_service",
            "method": "get_inventory",
            "parameters": {{}},
            "dependencies": []
        }},
        {{
            "id": "task2",
            "description": "åœ¨åº«ãƒªã‚¹ãƒˆã«åŸºã¥ãã€LLMã«ã‚ˆã‚‹ç‹¬å‰µçš„ãªçŒ®ç«‹ã‚’ææ¡ˆã™ã‚‹",
            "service": "recipe_service",
            "method": "generate_menu_plan",
            "parameters": {{ "inventory_items": "task1.result", "user_id": "user123" }},
            "dependencies": ["task1"]
        }},
        {{
            "id": "task3",
            "description": "åœ¨åº«ãƒªã‚¹ãƒˆã«åŸºã¥ãã€RAGã‚’ä½¿ç”¨ã—ã¦éå»ã®çŒ®ç«‹å±¥æ­´ã‹ã‚‰é¡ä¼¼çŒ®ç«‹ã‚’æ¤œç´¢ã™ã‚‹",
            "service": "recipe_service",
            "method": "search_menu_from_rag",
            "parameters": {{ "inventory_items": "task1.result", "user_id": "user123" }},
            "dependencies": ["task1"]
        }},
        {{
            "id": "task4",
            "description": "ææ¡ˆã•ã‚ŒãŸçŒ®ç«‹ã®ãƒ¬ã‚·ãƒ”ã‚’Webæ¤œç´¢ã—ã¦è©³ç´°æƒ…å ±ã‚’å–å¾—ã™ã‚‹",
            "service": "recipe_service",
            "method": "search_recipes_from_web",
            "parameters": {{ 
                "recipe_titles": ["task2.result.main_dish", "task2.result.side_dish", "task3.result.main_dish", "task3.result.side_dish"],
                "menu_categories": ["main_dish", "side_dish", "main_dish", "side_dish"],
                "menu_source": "mixed"
            }},
            "dependencies": ["task2", "task3"]
        }}
    ]
}}

**ä¾å­˜é–¢ä¿‚ã®ãƒ«ãƒ¼ãƒ«**:
- å„ã‚¿ã‚¹ã‚¯ã«ã¯ä¸€æ„ã®IDï¼ˆtask1, task2, ...ï¼‰ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚
- `dependencies` ã«ã¯ã€å®Ÿè¡Œå‰ã«å®Œäº†ã—ã¦ã„ã‚‹ã¹ãã‚¿ã‚¹ã‚¯ã®IDã‚’ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
- ä¾å­˜é–¢ä¿‚ãŒãªã„å ´åˆã¯ç©ºé…åˆ— `[]` ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

**æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã®å ´åˆ**:
- ã‚¿ã‚¹ã‚¯ã¯ç”Ÿæˆã›ãšã€ç©ºã®é…åˆ— `{{"tasks": []}}` ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
        return planning_prompt
    
    async def _call_openai_api(self, prompt: str) -> str:
        """
        OpenAI APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        
        Args:
            prompt: é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            if not self.openai_client:
                raise Exception("OpenAI client not initialized")
            
            self.logger.info(f"ğŸ”§ [LLMService] Calling OpenAI API with model: {self.openai_model}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆå…¨æ–‡è¡¨ç¤ºï¼‰
            log_prompt_with_tokens(prompt, max_tokens=2000, logger_name="service.llm", show_full_prompt=True)
            
            response = await self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªã‚¿ã‚¹ã‚¯åˆ†è§£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.openai_temperature,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            self.logger.info(f"âœ… [LLMService] OpenAI API response received: {len(content)} characters")
            
            # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ”¹è¡Œä»˜ãã§ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"ğŸ“„ [LLMService] LLM Response:\n{content}")
            
            return content
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] OpenAI API call failed: {e}")
            raise
    
    def _parse_llm_response(self, response: str) -> List[Dict[str, Any]]:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æã—ã¦ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            response: LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        
        Returns:
            è§£æã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Parsing LLM response")
            
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆ```json```ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ï¼‰
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "```" in response:
                start = response.find("```") + 3
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response.strip()
            
            # JSONè§£æ
            result = json.loads(json_str)
            tasks = result.get("tasks", [])
            
            self.logger.info(f"âœ… [LLMService] Parsed {len(tasks)} tasks from LLM response")
            return tasks
            
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ [LLMService] JSON parsing failed: {e}")
            self.logger.error(f"Response content: {response}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error parsing LLM response: {e}")
            return []
    
    def _convert_to_task_format(self, tasks: List[Dict[str, Any]], user_id: str) -> List[Dict[str, Any]]:
        """
        LLMã‚¿ã‚¹ã‚¯ã‚’ActionPlannerãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
        
        Args:
            tasks: LLMã‹ã‚‰å–å¾—ã—ãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            å¤‰æ›ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Converting {len(tasks)} tasks to ActionPlanner format")
            
            converted_tasks = []
            for task in tasks:
                # user_idã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
                parameters = task.get("parameters", {})
                if "user_id" not in parameters:
                    parameters["user_id"] = user_id
                
                converted_task = {
                    "service": task.get("service"),
                    "method": task.get("method"),
                    "parameters": parameters,
                    "dependencies": task.get("dependencies", [])
                }
                converted_tasks.append(converted_task)
            
            self.logger.info(f"âœ… [LLMService] Converted {len(converted_tasks)} tasks successfully")
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error converting tasks: {e}")
            return []
    
    async def decompose_tasks(
        self, 
        user_request: str, 
        available_tools: List[str], 
        user_id: str
    ) -> List[Dict[str, Any]]:
        """
        å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†è§£
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            åˆ†è§£ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Decomposing tasks for user: {user_id}")
            self.logger.info(f"ğŸ“ [LLMService] User request: '{user_request}'")
            
            # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not self.openai_client:
                self.logger.warning("âš ï¸ [LLMService] OpenAI client not available, using fallback")
                return self._get_fallback_tasks(user_id)
            
            # 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self._build_planning_prompt(user_request)
            
            # 2. OpenAI APIå‘¼ã³å‡ºã—
            response = await self._call_openai_api(prompt)
            
            # 3. JSONè§£æ
            tasks = self._parse_llm_response(response)
            
            # 4. ã‚¿ã‚¹ã‚¯å½¢å¼ã«å¤‰æ›
            converted_tasks = self._convert_to_task_format(tasks, user_id)
            
            # ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"âœ… [LLMService] Tasks decomposed successfully: {len(converted_tasks)} tasks")
            for i, task in enumerate(converted_tasks, 1):
                self.logger.info(f"ğŸ“‹ [LLMService] Task {i}:")
                self.logger.info(f"  Service: {task.get('service')}")
                self.logger.info(f"  Method: {task.get('method')}")
                self.logger.info(f"  Parameters: {task.get('parameters')}")
                self.logger.info(f"  Dependencies: {task.get('dependencies')}")
            
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in decompose_tasks: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._get_fallback_tasks(user_id)
    
    def _get_fallback_tasks(self, user_id: str) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚¿ã‚¹ã‚¯ï¼ˆLLMå‘¼ã³å‡ºã—å¤±æ•—æ™‚ï¼‰
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ğŸ”„ [LLMService] Using fallback tasks for user: {user_id}")
        
        return [
            {
                "service": "InventoryService",
                "method": "get_inventory",
                "parameters": {"user_id": user_id},
                "dependencies": []
            }
        ]
    
    async def format_response(
        self, 
        results: Dict[str, Any]
    ) -> str:
        """
        æœ€çµ‚å›ç­”æ•´å½¢
        
        Args:
            results: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœè¾æ›¸ (task1, task2, task3, task4)
        
        Returns:
            æ•´å½¢ã•ã‚ŒãŸå›ç­”
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Formatting response for {len(results)} results")
            
            # task4ã®Webæ¤œç´¢çµæœã‚’å–å¾—
            web_recipes = []
            if "task4" in results and results["task4"].get("success"):
                web_data = results["task4"].get("result", {}).get("data", [])
                web_recipes = web_data
            
            # task2ã¨task3ã®çŒ®ç«‹ã‚’å–å¾—
            llm_menu = {}
            rag_menu = {}
            if "task2" in results and results["task2"].get("success"):
                llm_menu = results["task2"].get("result", {}).get("data", {})
            if "task3" in results and results["task3"].get("success"):
                rag_menu = results["task3"].get("result", {}).get("data", {})
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ§‹ç¯‰
            response_parts = []
            
            # çŒ®ç«‹ææ¡ˆ
            if llm_menu:
                response_parts.append("ğŸ½ï¸ **LLMçŒ®ç«‹ææ¡ˆ**")
                response_parts.append(f"ä¸»èœ: {llm_menu.get('main_dish', 'N/A')}")
                response_parts.append(f"å‰¯èœ: {llm_menu.get('side_dish', 'N/A')}")
                response_parts.append(f"æ±ç‰©: {llm_menu.get('soup', 'N/A')}")
                response_parts.append("")
            
            if rag_menu:
                response_parts.append("ğŸ” **RAGçŒ®ç«‹ææ¡ˆ**")
                response_parts.append(f"ä¸»èœ: {rag_menu.get('main_dish', 'N/A')}")
                response_parts.append(f"å‰¯èœ: {rag_menu.get('side_dish', 'N/A')}")
                response_parts.append(f"æ±ç‰©: {rag_menu.get('soup', 'N/A')}")
                response_parts.append("")
            
            # Webæ¤œç´¢çµæœï¼ˆè©³ç´°åˆ†é¡å¯¾å¿œï¼‰
            if web_recipes:
                response_parts.append("ğŸŒ **ãƒ¬ã‚·ãƒ”æ¤œç´¢çµæœ**")
                
                # LLMçŒ®ç«‹ã®çµæœ
                llm_menu = web_recipes.get("llm_menu", {})
                if any(llm_menu.values()):
                    response_parts.append("")
                    response_parts.append("ğŸ½ï¸ **LLMçŒ®ç«‹ææ¡ˆ**")
                    
                    for category, data in llm_menu.items():
                        if data.get("title") and data.get("recipes"):
                            category_emoji = {"main_dish": "ğŸ¥©", "side_dish": "ğŸ¥¬", "soup": "ğŸ²"}.get(category, "ğŸ½ï¸")
                            response_parts.append(f"{category_emoji} **{category.replace('_', ' ').title()}: {data['title']}**")
                            
                            for i, recipe in enumerate(data["recipes"][:3], 1):  # ä¸Šä½3ä»¶ã®ã¿
                                response_parts.append(f"{i}. {recipe.get('title', 'N/A')}")
                                response_parts.append(f"   URL: {recipe.get('url', 'N/A')}")
                                response_parts.append(f"   èª¬æ˜: {recipe.get('description', 'N/A')[:100]}...")
                                response_parts.append("")
                
                # RAGçŒ®ç«‹ã®çµæœ
                rag_menu = web_recipes.get("rag_menu", {})
                if any(rag_menu.values()):
                    response_parts.append("")
                    response_parts.append("ğŸ” **RAGçŒ®ç«‹ææ¡ˆ**")
                    
                    for category, data in rag_menu.items():
                        if data.get("title") and data.get("recipes"):
                            category_emoji = {"main_dish": "ğŸ¥©", "side_dish": "ğŸ¥¬", "soup": "ğŸ²"}.get(category, "ğŸ½ï¸")
                            response_parts.append(f"{category_emoji} **{category.replace('_', ' ').title()}: {data['title']}**")
                            
                            for i, recipe in enumerate(data["recipes"][:3], 1):  # ä¸Šä½3ä»¶ã®ã¿
                                response_parts.append(f"{i}. {recipe.get('title', 'N/A')}")
                                response_parts.append(f"   URL: {recipe.get('url', 'N/A')}")
                                response_parts.append(f"   èª¬æ˜: {recipe.get('description', 'N/A')[:100]}...")
                                response_parts.append("")
            
            if not response_parts:
                return "ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸãŒã€çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            final_response = "\n".join(response_parts)
            self.logger.info(f"âœ… [LLMService] Response formatted successfully")
            
            return final_response
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in format_response: {e}")
            return "ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸãŒã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    async def solve_constraints(
        self, 
        candidates: List[Dict], 
        constraints: Dict
    ) -> Dict:
        """
        åˆ¶ç´„è§£æ±ºï¼ˆå­ãƒ•ã‚¡ã‚¤ãƒ«å§”è­²ï¼‰
        
        Args:
            candidates: å€™è£œãƒªã‚¹ãƒˆ
            constraints: åˆ¶ç´„æ¡ä»¶
        
        Returns:
            åˆ¶ç´„è§£æ±ºçµæœ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Solving constraints for {len(candidates)} candidates")
            
            # TODO: å®Ÿéš›ã®åˆ¶ç´„è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…
            result = {
                "selected": candidates[0] if candidates else {},
                "reason": "åˆ¶ç´„è§£æ±ºã«ã‚ˆã‚Šé¸æŠã•ã‚Œã¾ã—ãŸ"
            }
            
            self.logger.info(f"âœ… [LLMService] Constraints solved successfully")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in solve_constraints: {e}")
            return {"selected": {}, "reason": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"}
    
    def get_available_tools_description(self) -> str:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®èª¬æ˜ã‚’å–å¾—
        
        Returns:
            ãƒ„ãƒ¼ãƒ«èª¬æ˜ã®æ–‡å­—åˆ—
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Getting available tools description")
            
            # TODO: ServiceCoordinatorçµŒç”±ã§å–å¾—ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£äºˆå®š
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…
            description_text = "åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n"
            description_text += "- inventory_list: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—\n"
            description_text += "- generate_menu_plan: åœ¨åº«é£Ÿæã‹ã‚‰çŒ®ç«‹æ§‹æˆã‚’ç”Ÿæˆ\n"
            
            self.logger.info(f"âœ… [LLMService] Tools description generated successfully")
            return description_text
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in get_available_tools_description: {e}")
            return "ãƒ„ãƒ¼ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    def create_dynamic_prompt(
        self, 
        base_prompt: str, 
        tool_descriptions: str,
        user_context: Dict[str, Any]
    ) -> str:
        """
        å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        
        Args:
            base_prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            tool_descriptions: ãƒ„ãƒ¼ãƒ«èª¬æ˜
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Creating dynamic prompt")
            
            # å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            dynamic_prompt = f"""
{base_prompt}

{tool_descriptions}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_context.get('user_id', 'N/A')}
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {user_context.get('session_id', 'N/A')}
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»: {user_context.get('timestamp', 'N/A')}
"""
            
            self.logger.info(f"âœ… [LLMService] Dynamic prompt created successfully")
            
            return dynamic_prompt
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in create_dynamic_prompt: {e}")
            return base_prompt
