#!/usr/bin/env python3
"""
LLMService - LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹

LLMå‘¼ã³å‡ºã—ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹
åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦LLMæ©Ÿèƒ½ã‚’æä¾›
"""

from typing import Dict, Any, List
from config.loggers import GenericLogger
from .llm.prompt_manager import PromptManager
from .llm.response_processor import ResponseProcessor
from .llm.llm_client import LLMClient


class LLMService:
    """LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = GenericLogger("service", "llm")
        
        # åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
        self.prompt_manager = PromptManager()
        self.response_processor = ResponseProcessor()
        self.llm_client = LLMClient()
    
    async def decompose_tasks(
        self, 
        user_request: str, 
        available_tools: List[str], 
        user_id: str
    ) -> List[Dict[str, Any]]:
        """
        å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†è§£
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            åˆ†è§£ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Decomposing tasks for user: {user_id}")
            self.logger.info(f"ğŸ“ [LLMService] User request: '{user_request}'")
            
            # 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self.prompt_manager.build_planning_prompt(user_request)
            
            # 2. OpenAI APIå‘¼ã³å‡ºã—
            response = await self.llm_client.call_openai_api(prompt)
            
            # 3. JSONè§£æ
            tasks = self.response_processor.parse_llm_response(response)
            
            # 4. ã‚¿ã‚¹ã‚¯å½¢å¼ã«å¤‰æ›
            converted_tasks = self.response_processor.convert_to_task_format(tasks, user_id)
            
            # ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"âœ… [LLMService] Tasks decomposed successfully: {len(converted_tasks)} tasks")
            for i, task in enumerate(converted_tasks, 1):
                self.logger.info(f"ğŸ“‹ [LLMService] Task {i}:")
                self.logger.info(f"  Service: {task.get('service')}")
                self.logger.info(f"  Method: {task.get('method')}")
                self.logger.info(f"  Parameters: {task.get('parameters')}")
                self.logger.info(f"  Dependencies: {task.get('dependencies')}")
            
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in decompose_tasks: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self.llm_client.get_fallback_tasks(user_id)
    
    async def format_response(
        self, 
        results: Dict[str, Any]
    ) -> str:
        """
        æœ€çµ‚å›ç­”æ•´å½¢
        
        Args:
            results: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœè¾æ›¸ (task1, task2, task3, task4)
        
        Returns:
            æ•´å½¢ã•ã‚ŒãŸå›ç­”
        """
        return self.response_processor.format_final_response(results)
    
    async def solve_constraints(
        self, 
        candidates: List[Dict], 
        constraints: Dict
    ) -> Dict:
        """
        åˆ¶ç´„è§£æ±ºï¼ˆå­ãƒ•ã‚¡ã‚¤ãƒ«å§”è­²ï¼‰
        
        Args:
            candidates: å€™è£œãƒªã‚¹ãƒˆ
            constraints: åˆ¶ç´„æ¡ä»¶
        
        Returns:
            åˆ¶ç´„è§£æ±ºçµæœ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Solving constraints for {len(candidates)} candidates")
            
            # TODO: å®Ÿéš›ã®åˆ¶ç´„è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…
            result = {
                "selected": candidates[0] if candidates else {},
                "reason": "åˆ¶ç´„è§£æ±ºã«ã‚ˆã‚Šé¸æŠã•ã‚Œã¾ã—ãŸ"
            }
            
            self.logger.info(f"âœ… [LLMService] Constraints solved successfully")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in solve_constraints: {e}")
            return {"selected": {}, "reason": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"}
    
    def get_available_tools_description(self) -> str:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®èª¬æ˜ã‚’å–å¾—
        
        Returns:
            ãƒ„ãƒ¼ãƒ«èª¬æ˜ã®æ–‡å­—åˆ—
        """
        return self.llm_client.get_available_tools_description()
    
    def create_dynamic_prompt(
        self, 
        base_prompt: str, 
        tool_descriptions: str,
        user_context: Dict[str, Any]
    ) -> str:
        """
        å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        
        Args:
            base_prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            tool_descriptions: ãƒ„ãƒ¼ãƒ«èª¬æ˜
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        return self.prompt_manager.create_dynamic_prompt(base_prompt, tool_descriptions, user_context)
