#!/usr/bin/env python3
"""
LLMService - LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹

LLMå‘¼ã³å‡ºã—ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹
åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦LLMæ©Ÿèƒ½ã‚’æä¾›
"""

from typing import Dict, Any, List, Optional
from config.loggers import GenericLogger
from .llm.prompt_manager import PromptManager
from .llm.response_processor import ResponseProcessor
from .llm.llm_client import LLMClient


class LLMService:
    """LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = GenericLogger("service", "llm")
        
        # åˆ†å‰²ã•ã‚ŒãŸã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
        self.prompt_manager = PromptManager()
        self.response_processor = ResponseProcessor()
        self.llm_client = LLMClient()
    
    async def decompose_tasks(
        self, 
        user_request: str, 
        available_tools: List[str], 
        user_id: str,
        sse_session_id: str = None
    ) -> List[Dict[str, Any]]:
        """
        å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†è§£
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            åˆ†è§£ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Decomposing tasks for user: {user_id}")
            self.logger.info(f"ğŸ“ [LLMService] User request: '{user_request}'")
            
            # 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆPhase 1F: sse_session_idã‚’æ¸¡ã™ï¼‰
            prompt = self.prompt_manager.build_planning_prompt(user_request, sse_session_id)
            
            # 2. OpenAI APIå‘¼ã³å‡ºã—
            response = await self.llm_client.call_openai_api(prompt)
            
            # 3. JSONè§£æ
            tasks = self.response_processor.parse_llm_response(response)
            
            # 4. ã‚¿ã‚¹ã‚¯å½¢å¼ã«å¤‰æ›
            converted_tasks = self.response_processor.convert_to_task_format(tasks, user_id)
            
            # ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"âœ… [LLMService] Tasks decomposed successfully: {len(converted_tasks)} tasks")
            for i, task in enumerate(converted_tasks, 1):
                self.logger.info(f"ğŸ“‹ [LLMService] Task {i}:")
                self.logger.info(f"  Service: {task.get('service')}")
                self.logger.info(f"  Method: {task.get('method')}")
                self.logger.info(f"  Parameters: {task.get('parameters')}")
                self.logger.info(f"  Dependencies: {task.get('dependencies')}")
            
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in decompose_tasks: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self.llm_client.get_fallback_tasks(user_id)
    
    async def format_response(
        self, 
        results: Dict[str, Any],
        sse_session_id: str = None
    ) -> tuple[str, Optional[Dict[str, Any]]]:
        """
        æœ€çµ‚å›ç­”æ•´å½¢
        
        Args:
            results: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœè¾æ›¸ (task1, task2, task3, task4)
            sse_session_id: SSEã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        
        Returns:
            (æ•´å½¢ã•ã‚ŒãŸå›ç­”, JSONå½¢å¼ã®ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿)
        """
        response, menu_data = await self.response_processor.format_final_response(results, sse_session_id)
        self.logger.info(f"ğŸ” [LLMService] Menu data received: {menu_data is not None}")
        if menu_data:
            self.logger.info(f"ğŸ“Š [LLMService] Menu data size: {len(str(menu_data))} characters")
        return response, menu_data
    
    def create_dynamic_prompt(
        self, 
        base_prompt: str, 
        tool_descriptions: str,
        user_context: Dict[str, Any]
    ) -> str:
        """
        å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        
        Args:
            base_prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            tool_descriptions: ãƒ„ãƒ¼ãƒ«èª¬æ˜
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        return self.prompt_manager.create_dynamic_prompt(base_prompt, tool_descriptions, user_context)
