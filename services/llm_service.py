#!/usr/bin/env python3
"""
LLMService - LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹

LLMå‘¼ã³å‡ºã—ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆãƒ»ç®¡ç†ã¨å‹•çš„ãƒ„ãƒ¼ãƒ«å–å¾—ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‹•çš„åŸ‹ã‚è¾¼ã¿ã‚’æä¾›
"""

import os
import json
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from openai import AsyncOpenAI
from config.loggers import GenericLogger, log_prompt_with_tokens

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


class LLMService:
    """LLMå‘¼ã³å‡ºã—ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = GenericLogger("service", "llm")
        
        # OpenAIè¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.openai_temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.8"))
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if self.openai_api_key:
            self.openai_client = AsyncOpenAI(api_key=self.openai_api_key)
            self.logger.info(f"âœ… [LLMService] OpenAI client initialized with model: {self.openai_model}")
        else:
            self.openai_client = None
            self.logger.warning("âš ï¸ [LLMService] OPENAI_API_KEY not found, LLM calls will be disabled")
    
    def _build_planning_prompt(self, user_request: str) -> str:
        """
        ã‚¿ã‚¹ã‚¯åˆ†è§£ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        planning_prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã‚’åˆ†æã—ã€é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚: "{user_request}"

åˆ©ç”¨å¯èƒ½ãªã‚µãƒ¼ãƒ“ã‚¹ã¨æ©Ÿèƒ½:

- **inventory_service**: åœ¨åº«ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹
  - `get_inventory()`: ç¾åœ¨ã®å…¨åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚
  - `add_inventory(item_name: str, quantity: float, ...)`: åœ¨åº«ã«æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ ã—ã¾ã™ã€‚
  - `update_inventory(item_identifier: str, updates: dict, strategy: str)`: åœ¨åº«æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚strategyã«ã¯ 'by_id', 'by_name', 'by_name_oldest', 'by_name_latest' ãŒæŒ‡å®šå¯èƒ½ã§ã™ã€‚
  - `delete_inventory(item_identifier: str, strategy: str)`: åœ¨åº«ã‚’å‰Šé™¤ã—ã¾ã™ã€‚strategyã«ã¯ 'by_id', 'by_name', 'by_name_oldest', 'by_name_latest' ãŒæŒ‡å®šå¯èƒ½ã§ã™ã€‚

- **recipe_service**: ãƒ¬ã‚·ãƒ”ãƒ»çŒ®ç«‹ã‚µãƒ¼ãƒ“ã‚¹
  - `generate_menu_plan(inventory_items: list, user_id: str, ...)`: åœ¨åº«ãƒªã‚¹ãƒˆã«åŸºã¥ãã€æœ€é©ãªçŒ®ç«‹ï¼ˆä¸»èœãƒ»å‰¯èœãƒ»æ±ç‰©ï¼‰ã‚’ææ¡ˆã—ã¾ã™ã€‚å†…éƒ¨ã§LLMã«ã‚ˆã‚‹ç‹¬å‰µçš„ãªææ¡ˆã¨RAGã«ã‚ˆã‚‹ä¼çµ±çš„ãªææ¡ˆã‚’æ¯”è¼ƒæ¤œè¨ã—ã¾ã™ã€‚
  - `search_recipes(title: str)`: æŒ‡å®šã•ã‚ŒãŸæ–™ç†åã®ãƒ¬ã‚·ãƒ”ã‚’Webæ¤œç´¢ã—ã€URLã‚’å«ã‚€è©³ç´°æƒ…å ±ã‚’è¿”ã—ã¾ã™ã€‚
  - `check_cooking_history(user_id: str, ...)`: éå»ã®æ–™ç†å±¥æ­´ã‚’å–å¾—ã—ã¾ã™ã€‚

- **session_service**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆé€šå¸¸ã¯ç›´æ¥å‘¼ã³å‡ºã—ä¸è¦ï¼‰


**æœ€é‡è¦ãƒ«ãƒ¼ãƒ«: çŒ®ç«‹ç”Ÿæˆã®éš›ã®ã‚¿ã‚¹ã‚¯æ§‹æˆ**
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ãŒã€ŒçŒ®ç«‹ã€ã‚„ã€Œãƒ¬ã‚·ãƒ”ã€ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹å ´åˆã€å¿…ãšä»¥ä¸‹ã®2æ®µéšã®ã‚¿ã‚¹ã‚¯æ§‹æˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:
1. `inventory_service.get_inventory()` ã‚’å‘¼ã³å‡ºã—ã€ç¾åœ¨ã®åœ¨åº«ã‚’ã™ã¹ã¦å–å¾—ã™ã‚‹ã€‚
2. `recipe_service.generate_menu_plan()` ã‚’å‘¼ã³å‡ºã™ã€‚ãã®éš›ã€ã‚¹ãƒ†ãƒƒãƒ—1ã§å–å¾—ã—ãŸåœ¨åº«æƒ…å ±ã‚’ `inventory_items` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®šã™ã‚‹ã€‚

**åœ¨åº«è¿½åŠ ã¨çŒ®ç«‹ç”Ÿæˆã‚’åŒæ™‚ã«è¦æ±‚ã•ã‚ŒãŸå ´åˆã®ã‚¿ã‚¹ã‚¯æ§‹æˆ**:
1. `inventory_service.add_inventory()` ã§ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ ã™ã‚‹ã€‚ï¼ˆè¤‡æ•°ã‚¢ã‚¤ãƒ†ãƒ ã®å ´åˆã¯ä¸¦åˆ—å®Ÿè¡Œï¼‰
2. `inventory_service.get_inventory()` ã‚’å‘¼ã³å‡ºã—ã€è¿½åŠ å¾Œã‚’å«ã‚ãŸæœ€æ–°ã®åœ¨åº«ã‚’å–å¾—ã™ã‚‹ã€‚
3. `recipe_service.generate_menu_plan()` ã‚’å‘¼ã³å‡ºã—ã€ã‚¹ãƒ†ãƒƒãƒ—2ã®çµæœã‚’æ³¨å…¥ã™ã‚‹ã€‚

**æ›–æ˜§ãªåœ¨åº«æ“ä½œã®æŒ‡ç¤ºã«ã¤ã„ã¦**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œå¤ã„æ–¹ã€ã€Œæœ€æ–°ã€ãªã©ã‚’æ˜ç¤ºã—ãªã„é™ã‚Šã€`update_inventory` ã‚„ `delete_inventory` ã® `strategy` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ `'by_name'` ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µãƒ¼ãƒ“ã‚¹å±¤ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç¢ºèªãƒ—ãƒ­ã‚»ã‚¹ãŒèµ·å‹•ã—ã¾ã™ã€‚
- ä¾‹: ã€Œç‰›ä¹³ã‚’å‰Šé™¤ã—ã¦ã€ â†’ `delete_inventory(item_identifier='ç‰›ä¹³', strategy='by_name')`
- ä¾‹: ã€Œå¤ã„ç‰›ä¹³ã‚’å‰Šé™¤ã—ã¦ã€ â†’ `delete_inventory(item_identifier='ç‰›ä¹³', strategy='by_name_oldest')`

**å‡ºåŠ›å½¢å¼**: å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã¯ç¦æ­¢ï¼‰ï¼š

{{
    "tasks": [
        {{
            "id": "task1",
            "description": "ã‚¿ã‚¹ã‚¯ã®è‡ªç„¶è¨€èªã§ã®èª¬æ˜",
            "service": "å‘¼ã³å‡ºã™ã‚µãƒ¼ãƒ“ã‚¹å",
            "method": "å‘¼ã³å‡ºã™ãƒ¡ã‚½ãƒƒãƒ‰å",
            "parameters": {{ "key": "value" }},
            "dependencies": []
        }}
    ]
}}

**ä¾å­˜é–¢ä¿‚ã®ãƒ«ãƒ¼ãƒ«**:
- å„ã‚¿ã‚¹ã‚¯ã«ã¯ä¸€æ„ã®IDï¼ˆtask1, task2, ...ï¼‰ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚
- `dependencies` ã«ã¯ã€å®Ÿè¡Œå‰ã«å®Œäº†ã—ã¦ã„ã‚‹ã¹ãã‚¿ã‚¹ã‚¯ã®IDã‚’ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
- ä¾å­˜é–¢ä¿‚ãŒãªã„å ´åˆã¯ç©ºé…åˆ— `[]` ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

**æŒ¨æ‹¶ã‚„ä¸€èˆ¬çš„ãªä¼šè©±ã®å ´åˆ**:
- ã‚¿ã‚¹ã‚¯ã¯ç”Ÿæˆã›ãšã€ç©ºã®é…åˆ— `{{"tasks": []}}` ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
        return planning_prompt
    
    async def _call_openai_api(self, prompt: str) -> str:
        """
        OpenAI APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        
        Args:
            prompt: é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            if not self.openai_client:
                raise Exception("OpenAI client not initialized")
            
            self.logger.info(f"ğŸ”§ [LLMService] Calling OpenAI API with model: {self.openai_model}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆå…¨æ–‡è¡¨ç¤ºï¼‰
            log_prompt_with_tokens(prompt, max_tokens=2000, logger_name="service.llm", show_full_prompt=True)
            
            response = await self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªã‚¿ã‚¹ã‚¯åˆ†è§£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.openai_temperature,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            self.logger.info(f"âœ… [LLMService] OpenAI API response received: {len(content)} characters")
            
            # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ”¹è¡Œä»˜ãã§ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"ğŸ“„ [LLMService] LLM Response:\n{content}")
            
            return content
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] OpenAI API call failed: {e}")
            raise
    
    def _parse_llm_response(self, response: str) -> List[Dict[str, Any]]:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æã—ã¦ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            response: LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        
        Returns:
            è§£æã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Parsing LLM response")
            
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆ```json```ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ï¼‰
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "```" in response:
                start = response.find("```") + 3
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response.strip()
            
            # JSONè§£æ
            result = json.loads(json_str)
            tasks = result.get("tasks", [])
            
            self.logger.info(f"âœ… [LLMService] Parsed {len(tasks)} tasks from LLM response")
            return tasks
            
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ [LLMService] JSON parsing failed: {e}")
            self.logger.error(f"Response content: {response}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error parsing LLM response: {e}")
            return []
    
    def _convert_to_task_format(self, tasks: List[Dict[str, Any]], user_id: str) -> List[Dict[str, Any]]:
        """
        LLMã‚¿ã‚¹ã‚¯ã‚’ActionPlannerãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
        
        Args:
            tasks: LLMã‹ã‚‰å–å¾—ã—ãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            å¤‰æ›ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Converting {len(tasks)} tasks to ActionPlanner format")
            
            converted_tasks = []
            for task in tasks:
                # user_idã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
                parameters = task.get("parameters", {})
                if "user_id" not in parameters:
                    parameters["user_id"] = user_id
                
                converted_task = {
                    "service": task.get("service"),
                    "method": task.get("method"),
                    "parameters": parameters,
                    "dependencies": task.get("dependencies", [])
                }
                converted_tasks.append(converted_task)
            
            self.logger.info(f"âœ… [LLMService] Converted {len(converted_tasks)} tasks successfully")
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error converting tasks: {e}")
            return []
    
    async def decompose_tasks(
        self, 
        user_request: str, 
        available_tools: List[str], 
        user_id: str
    ) -> List[Dict[str, Any]]:
        """
        å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†è§£
        
        Args:
            user_request: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            åˆ†è§£ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Decomposing tasks for user: {user_id}")
            self.logger.info(f"ğŸ“ [LLMService] User request: '{user_request}'")
            
            # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not self.openai_client:
                self.logger.warning("âš ï¸ [LLMService] OpenAI client not available, using fallback")
                return self._get_fallback_tasks(user_id)
            
            # 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self._build_planning_prompt(user_request)
            
            # 2. OpenAI APIå‘¼ã³å‡ºã—
            response = await self._call_openai_api(prompt)
            
            # 3. JSONè§£æ
            tasks = self._parse_llm_response(response)
            
            # 4. ã‚¿ã‚¹ã‚¯å½¢å¼ã«å¤‰æ›
            converted_tasks = self._convert_to_task_format(tasks, user_id)
            
            # ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"âœ… [LLMService] Tasks decomposed successfully: {len(converted_tasks)} tasks")
            for i, task in enumerate(converted_tasks, 1):
                self.logger.info(f"ğŸ“‹ [LLMService] Task {i}:")
                self.logger.info(f"  Service: {task.get('service')}")
                self.logger.info(f"  Method: {task.get('method')}")
                self.logger.info(f"  Parameters: {task.get('parameters')}")
                self.logger.info(f"  Dependencies: {task.get('dependencies')}")
            
            return converted_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in decompose_tasks: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._get_fallback_tasks(user_id)
    
    def _get_fallback_tasks(self, user_id: str) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚¿ã‚¹ã‚¯ï¼ˆLLMå‘¼ã³å‡ºã—å¤±æ•—æ™‚ï¼‰
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ğŸ”„ [LLMService] Using fallback tasks for user: {user_id}")
        
        return [
            {
                "service": "InventoryService",
                "method": "get_inventory",
                "parameters": {"user_id": user_id},
                "dependencies": []
            }
        ]
    
    async def format_response(
        self, 
        results: List[Dict[str, Any]]
    ) -> str:
        """
        æœ€çµ‚å›ç­”æ•´å½¢ï¼ˆå­ãƒ•ã‚¡ã‚¤ãƒ«å§”è­²ï¼‰
        
        Args:
            results: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœãƒªã‚¹ãƒˆ
        
        Returns:
            æ•´å½¢ã•ã‚ŒãŸå›ç­”
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Formatting response for {len(results)} results")
            
            # TODO: å®Ÿéš›ã®å›ç­”æ•´å½¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…
            formatted_response = "ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
            
            self.logger.info(f"âœ… [LLMService] Response formatted successfully")
            
            return formatted_response
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in format_response: {e}")
            return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    async def solve_constraints(
        self, 
        candidates: List[Dict], 
        constraints: Dict
    ) -> Dict:
        """
        åˆ¶ç´„è§£æ±ºï¼ˆå­ãƒ•ã‚¡ã‚¤ãƒ«å§”è­²ï¼‰
        
        Args:
            candidates: å€™è£œãƒªã‚¹ãƒˆ
            constraints: åˆ¶ç´„æ¡ä»¶
        
        Returns:
            åˆ¶ç´„è§£æ±ºçµæœ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Solving constraints for {len(candidates)} candidates")
            
            # TODO: å®Ÿéš›ã®åˆ¶ç´„è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…
            result = {
                "selected": candidates[0] if candidates else {},
                "reason": "åˆ¶ç´„è§£æ±ºã«ã‚ˆã‚Šé¸æŠã•ã‚Œã¾ã—ãŸ"
            }
            
            self.logger.info(f"âœ… [LLMService] Constraints solved successfully")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in solve_constraints: {e}")
            return {"selected": {}, "reason": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"}
    
    def get_available_tools_description(self) -> str:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®èª¬æ˜ã‚’å–å¾—
        
        Returns:
            ãƒ„ãƒ¼ãƒ«èª¬æ˜ã®æ–‡å­—åˆ—
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Getting available tools description")
            
            # TODO: ServiceCoordinatorçµŒç”±ã§å–å¾—ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£äºˆå®š
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…
            description_text = "åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n"
            description_text += "- inventory_list: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—\n"
            description_text += "- generate_menu_plan: åœ¨åº«é£Ÿæã‹ã‚‰çŒ®ç«‹æ§‹æˆã‚’ç”Ÿæˆ\n"
            
            self.logger.info(f"âœ… [LLMService] Tools description generated successfully")
            return description_text
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in get_available_tools_description: {e}")
            return "ãƒ„ãƒ¼ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    def create_dynamic_prompt(
        self, 
        base_prompt: str, 
        tool_descriptions: str,
        user_context: Dict[str, Any]
    ) -> str:
        """
        å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        
        Args:
            base_prompt: ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            tool_descriptions: ãƒ„ãƒ¼ãƒ«èª¬æ˜
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        try:
            self.logger.info(f"ğŸ”§ [LLMService] Creating dynamic prompt")
            
            # å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            dynamic_prompt = f"""
{base_prompt}

{tool_descriptions}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_context.get('user_id', 'N/A')}
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {user_context.get('session_id', 'N/A')}
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»: {user_context.get('timestamp', 'N/A')}
"""
            
            self.logger.info(f"âœ… [LLMService] Dynamic prompt created successfully")
            
            return dynamic_prompt
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMService] Error in create_dynamic_prompt: {e}")
            return base_prompt
