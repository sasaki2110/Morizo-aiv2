#!/usr/bin/env python3
"""
LLMClient - LLM APIå‘¼ã³å‡ºã—

LLMServiceã‹ã‚‰åˆ†é›¢ã—ãŸLLM APIå‘¼ã³å‡ºã—å°‚ç”¨ã‚¯ãƒ©ã‚¹
OpenAI APIå‘¼ã³å‡ºã—ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’æ‹…å½“
"""

import os
from typing import Dict, Any, List
from dotenv import load_dotenv
from openai import AsyncOpenAI
from config.loggers import GenericLogger, log_prompt_with_tokens

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


class LLMClient:
    """LLM APIå‘¼ã³å‡ºã—ã‚¯ãƒ©ã‚¹"""
    
    MAX_TOKENS = 3000  # ãƒãƒƒã‚¯ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = GenericLogger("service", "llm.client")
        
        # OpenAIè¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.openai_temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.8"))
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if self.openai_api_key:
            self.openai_client = AsyncOpenAI(api_key=self.openai_api_key)
            self.logger.info(f"âœ… [LLMClient] OpenAI client initialized with model: {self.openai_model}")
        else:
            self.openai_client = None
            self.logger.warning("âš ï¸ [LLMClient] OPENAI_API_KEY not found, LLM calls will be disabled")
    
    async def call_openai_api(self, prompt: str) -> str:
        """
        OpenAI APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        
        Args:
            prompt: é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            if not self.openai_client:
                raise Exception("OpenAI client not initialized")
            
            self.logger.info(f"ğŸ”§ [LLMClient] Calling OpenAI API with model: {self.openai_model}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5è¡Œçœç•¥è¡¨ç¤ºï¼‰
            log_prompt_with_tokens(prompt, max_tokens=self.MAX_TOKENS, logger_name="service.llm")
            
            response = await self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªã‚¿ã‚¹ã‚¯åˆ†è§£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’é©åˆ‡ãªã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.openai_temperature,
                max_tokens=self.MAX_TOKENS
            )
            
            content = response.choices[0].message.content
            self.logger.info(f"âœ… [LLMClient] OpenAI API response received: {len(content)} characters")
            
            # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ”¹è¡Œä»˜ãã§ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"ğŸ“„ [LLMClient] LLM Response:\n{content}")
            
            return content
            
        except Exception as e:
            self.logger.error(f"âŒ [LLMClient] OpenAI API call failed: {e}")
            raise
    
    def get_fallback_tasks(self, user_id: str) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚¿ã‚¹ã‚¯ï¼ˆLLMå‘¼ã³å‡ºã—å¤±æ•—æ™‚ï¼‰
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ğŸ”„ [LLMClient] Using fallback tasks for user: {user_id}")
        
        return [
            {
                "service": "InventoryService",
                "method": "get_inventory",
                "parameters": {"user_id": user_id},
                "dependencies": []
            }
        ]
    
